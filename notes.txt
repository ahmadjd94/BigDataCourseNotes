Bdata notes :

lecture 1
__________________________________ 
datatypes :
	structured :SQL 
	unstructured : clear text
	semi-structured  XML

	in hadoop we can deal with all the types 

	hadoop components : 
	1 apache hive , apache Pig , apache Spark

	web1: static websites 
	web2: dynamic website
	web3: user-generated content (wikipedia,twitter)

	challeneges : 
	1 : storing huge data
	2 : processing huge data 

	the idea behind hadoop : storing and retrieving data from big data repo
	scalling up : increase computing power of the machine (EXPENSIVE)
	scalling horizontally : increase number of processing nodes (makes a cluster)
	a user may deal with a cluster as a single node 
	a cluster manages itself to distribute tasks between nodes (master nodes manages the cluster)

	gigabytes : ERP 
	Terabytes : CRM 
	Petabytes : WEB 
	Exabytes : bigdata , sensors ......

	charactaristics of BIG data : 3V's
	Volume : size of the data
		factors increasing the volume of data : 
			transactions , datastreaming from social media and increasing the number of sensors and machines connected
		problems : 
			Storage costs , relevance : determined through analysis (expensive) 

	velocity : how much data is coming  to the company. (rate at which new data is generated)
		MB GB TB ....
		problems with velocity : 
			reacting quickly to benefit from data.
			inconsistent data flow with peaks Daily or event-triggered.


	variety : types of data (structured , unstrcutured, semi-..)
		structured : SQL 
		semi-structured: JSON ,XML 
		problems : 
			how to gather data afrom heterogenic datatypes, link data and cleanse  data
			connect and build relations between 


Threats 
	lakes of data : data that are not connected and hard to integrate

Hadoop : makes datalakes centralized , and eases the analysis of data and allows distrbiuted proccessing using simple programming models , can scale out form single server to thousnads of machine 
hardware indepentent : doesnot rely on hrdware to provide high availability 

hadoop core : storage + compute
	YARN:managaes resources (CPU and RAM)
	HDFS : manages distribued storage 


___________________________________
lecture 2 : 
horton works sandbox : ready-made disk image for simulation 
support microsoft azure and AWS 
___________________________________

lecture 3 :  hadoop supports solr

machine learning : mahoot : based on hadoop 1 : bootleneck  in disk I/O
Mlib supported in hortonworks , support scala an python

why choose bigdata : because of bussiness outcomes : 
	data discovery

ETL : extract, transform, load :
	extract data from somewhere 
	transform to some usable form 
	load into cluster
transformation has the trend of predictive analysis
requires experts and depeneds on machine learning
why are you so retarded ?
because i can #motivation_speech
RETARD
E
T
A
R
D


hive : semi-sql command 
spark : in-memory data processing 
SQoop : can be used to integrate strcutred data into hadoop
storm : real time access
ambari : cluster mngmnt



YARN : benefits :


______________________________________
lecture 5



commands : scp , secure copy 
example :
	scp -P 2222 1.txt root@192.168.126.128:/root
	the previous command copies the file to the LFS (linux file system)
hadoop commands begin with hdfs

pcp : putty copy 

____________________
resetting admin password :
admbari-admin-password-reset 

_______________block size : default :128 MB


examples : 
file size: 200

2 blocks : 128 MB block and 72MB  

balancing clusters such that  3 replicas exist in the ecosystem 